{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - LSH\n",
    "\n",
    "Implement and apply LSH to identify similar news articles. This method identifies candidate pairs of news articles that are likely to have high Jaccard similarity.\n",
    "\n",
    "### Exercise 2.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/06 08:04:19 WARN Utils: Your hostname, nudibranch-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.55.165 instead (on interface wlp2s0)\n",
      "24/07/06 08:04:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/06 08:04:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import json\n",
    "import re\n",
    "import time \n",
    "import random\n",
    "import itertools\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Load the JSON data from the file\n",
    "sentences = {}\n",
    "with open('covid_news_small.json', 'rt') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)  # Load JSON object from line\n",
    "        \n",
    "        # Clean and split text into sentences\n",
    "        text = re.sub(r\"[\\r\\n]+|\\W+\", \" \", data[\"text\"]).strip()  # Remove newlines and non-word characters\n",
    "        \n",
    "        sentences[data[\"tweet_id\"]] = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shingles Generation\n",
    "\n",
    "Returns a list of shingles with a given length for a given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3  # Length of each shingle\n",
    "def generate_shingles(text: str) -> set[int]:\n",
    "    return {text[i:i+k] for i in range(len(text) - k + 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinHash Signatures Generation\n",
    "\n",
    "MinHashing aims to estimate the Jaccard similarity between sets. \n",
    "\n",
    "**Signature Generation:** Returns list of signatures for sets of shingles extracted from text data.\n",
    "\n",
    "**Universal Hashing:** Uses random coefficients a and b for universal hashing to compute hash values for each shingle for K hash functions (choosing the minimum hash value). This strategy is chosen to reduce collisions and evenly distribute shingles across hash buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants definition\n",
    "num_bands = 15 # Number of bands\n",
    "num_rows = 10 # Number of rows in the signature matrix\n",
    "p = 2**32 - 1 # A large prime number\n",
    "\n",
    "def choose_a_b():\n",
    "    a = random.randint(1, p - 1)\n",
    "    b = random.randint(0, p - 1)\n",
    "    return a, b\n",
    "\n",
    "def universal_hashing(hash_function_index):\n",
    "    a, b = choose_a_b()\n",
    "    length_shingles = 1_000_000_000 \n",
    "    hash_value = (a * int(hash_function_index) + b) % p % length_shingles\n",
    "    return hash_value\n",
    "\n",
    "def generate_signature(shingles) -> list[int]:\n",
    "    num_hashes = num_bands * num_rows  # K hash functions\n",
    "    sig = [float('inf')] * num_hashes  \n",
    "\n",
    "    for shingle in shingles:\n",
    "        for h in range(num_hashes):\n",
    "            hash_value = universal_hashing(h) \n",
    "            if hash_value < sig[h]:\n",
    "                sig[h] = hash_value\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bands Creation\n",
    "\n",
    "returns a list of bands for a given list of signatures\n",
    "each band is a hash value and this value is different for each band (offset)\n",
    "this is done to put each row of bands in a bucket without collisions from other rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bands(signature):\n",
    "    band_indices = [i % num_bands for i in signature]\n",
    "    return band_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark implementation of LSH\n",
    "\n",
    "Sparkâ€™s distributed computing capabilities to handle large-scale data processing in parallel, making it suitable for tasks like LSH where computation can be split across multiple nodes. Each node processes a subset of the data in parallel, ensuring efficient computation of shingles, MinHash signatures, bands, and hash buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  30.30948567390442\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from the sentences\n",
    "sentences_rdd = sc.parallelize(sentences)\n",
    "\n",
    "# Invert the signature matrix\n",
    "def invert(item):\n",
    "    doc_id, bands = item\n",
    "    return [(band, doc_id) for band in bands]\n",
    "\n",
    "start = time.time()\n",
    "lsh = ( sentences_rdd\n",
    "        .mapValues(generate_shingles)\n",
    "        .mapValues(generate_signature)\n",
    "        .mapValues(generate_bands)\n",
    "        .flatMap(invert)\n",
    "        .groupByKey()\n",
    "        .collect()\n",
    "        )\n",
    "print(\"Execution time: \", time.time() - start)\n",
    "\n",
    "buckets = { band: list(docs) for band, docs in lsh }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 elements of buckets:\n",
      "Band: nan, Docs: ['1']\n",
      "Band: nan, Docs: ['1']\n",
      "Band: nan, Docs: ['1']\n",
      "Band: nan, Docs: ['1']\n",
      "Band: nan, Docs: ['1']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 elements of buckets:\")\n",
    "for i, (band, docs) in enumerate(buckets.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Band: {band}, Docs: {docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate pairs identification\n",
    "\n",
    "The number of bands and rows should be parameters. Select a combination that finds as candidates at least 90% of pairs with 85% similarity and less than 5% of pairs with 60% similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m candidate_pairs\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Extract candidate pairs from buckets\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m candidate_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mget_candidate_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuckets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal candidates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidate_pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCandidate pairs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mget_candidate_pairs\u001b[0;34m(buckets)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_candidate_pairs\u001b[39m(buckets):\n\u001b[1;32m      2\u001b[0m     candidate_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m band, group \u001b[38;5;129;01min\u001b[39;00m buckets:\n\u001b[1;32m      5\u001b[0m         doc_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(group)\n\u001b[1;32m      6\u001b[0m         pairs_in_band \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mcombinations(doc_ids, \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "def get_candidate_pairs(buckets):\n",
    "    candidate_pairs = []\n",
    "\n",
    "    for band, group in buckets:\n",
    "        doc_ids = list(group)\n",
    "        pairs_in_band = list(itertools.combinations(doc_ids, 2))\n",
    "        candidate_pairs.extend(pairs_in_band)\n",
    "\n",
    "    return candidate_pairs\n",
    "\n",
    "# Extract candidate pairs from buckets\n",
    "candidate_pairs = get_candidate_pairs(buckets)\n",
    "print(f\"Total candidates: {len(candidate_pairs)}\")\n",
    "\n",
    "print(\"Candidate pairs:\")\n",
    "for i,pair in enumerate(candidate_pairs):\n",
    "    print(pair)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "Implement a function that, given a news article, returns all other news articles that are at least 85% similar. You should make use of a pre-processed set of candidate pairs, obtained by LSH, and return only the ones that have Jaccard similarityâ€“considering the shinglesâ€“above 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "\n",
    "Jaccard Similarity is a measure of how similar two sets are.\n",
    "The Jaccard Similarity of two sets is the ratio of the size of the intersection of the sets to the size of the union of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def retrieve_similar_articles(article_id, candidate_pairs, shingles_dict):\n",
    "    \"\"\"\n",
    "    Retrieve articles similar to the given article_id from candidate_pairs.\n",
    "    Returns a list of similar article IDs.\n",
    "    \"\"\"\n",
    "    similar_articles = []\n",
    "    for pair in candidate_pairs:\n",
    "        if article_id in pair:\n",
    "            other_article_id = pair[0] if pair[0] != article_id else pair[1]\n",
    "            similarity = jaccard_similarity(set(shingles_dict[article_id]), set(shingles_dict[other_article_id]))\n",
    "            if similarity >= 0.85:\n",
    "                similar_articles.append((other_article_id, similarity))\n",
    "    return similar_articles\n",
    "\n",
    "# choose random document\n",
    "doc_id = random.choice(list(sentences.keys()))\n",
    "doc_text = sentences[doc_id]\n",
    "shingles = generate_shingles(doc_text)\n",
    "signature = generate_signature(shingles)\n",
    "bands = generate_bands(signature)\n",
    "doc_buckets = [str(bucket) for bucket in bands]\n",
    "\n",
    "\n",
    "print(f\"Similar articles to document {doc_id} with similarity >= 0.85:\")\n",
    "for i, (doc_id, similarity) in enumerate(workflow):\n",
    "    print(f\"{doc_id}: {similarity:.4f}\")\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 \n",
    "\n",
    "Using a sample of the dataset, evaluate the LSH method by calculating the Jaccard similarities\n",
    "and obtaining the percentage of false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lsh(candidate_pairs, shingles_dict, sample_size=100):\n",
    "    \"\"\"\n",
    "    Evaluate LSH method by calculating Jaccard similarities and obtaining percentages of false positives and false negatives.\n",
    "    Returns average false positives and false negatives over multiple samples.\n",
    "    \"\"\"\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Sample random articles for evaluation\n",
    "    sample_articles = random.sample(list(shingles_dict.keys()), sample_size)\n",
    "    \n",
    "    for article_id in sample_articles:\n",
    "        article_shingles = set(shingles_dict[article_id])\n",
    "        for pair in candidate_pairs:\n",
    "            if article_id in pair:\n",
    "                other_article_id = pair[0] if pair[0] != article_id else pair[1]\n",
    "                other_article_shingles = set(shingles_dict[other_article_id])\n",
    "                similarity = jaccard_similarity(article_shingles, other_article_shingles)\n",
    "                \n",
    "                # Assuming ground truth or manually checking for false positives and negatives\n",
    "                if similarity < 0.85 and pair in candidate_pairs:\n",
    "                    false_positives += 1\n",
    "                elif similarity >= 0.85 and pair not in candidate_pairs:\n",
    "                    false_negatives += 1\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_pairs = len(candidate_pairs)\n",
    "    total_samples = sample_size * len(candidate_pairs)\n",
    "    percent_false_positives = (false_positives / total_samples) * 100\n",
    "    percent_false_negatives = (false_negatives / total_samples) * 100\n",
    "    \n",
    "    return percent_false_positives, percent_false_negatives\n",
    "\n",
    "# Example usage\n",
    "num_samples = 10\n",
    "total_false_positives = 0\n",
    "total_false_negatives = 0\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    fp, fn = evaluate_lsh()\n",
    "    total_false_positives += fp\n",
    "    total_false_negatives += fn\n",
    "\n",
    "average_false_positives = total_false_positives / num_samples\n",
    "average_false_negatives = total_false_negatives / num_samples\n",
    "\n",
    "print(f\"Average False Positives: {average_false_positives:.2f}%\")\n",
    "print(f\"Average False Negatives: {average_false_negatives:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
